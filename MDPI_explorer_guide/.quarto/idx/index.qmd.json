{"title":"Get started! A guide to MDPIexploreR","markdown":{"yaml":{"title":"Get started! A guide to MDPIexploreR"},"headingText":"Background","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n<!-- badges: start -->\n[![](https://github.com/pgomba/MDPI_explorer/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/pgomba/MDPI_explorer/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n\nEver changing scientific publishing strategies shape academic communications.\n\nTo date, MDPI is the largest publisher of Open Access articles in the world and top-3 overall publisher (Right after Elsevier and SpringerNature). \"[The Strain on Scientific Publishing](https://arxiv.org/abs/2309.15884)\" highlights them as a frequent outlier for several metrics, but also as one of the most transparent major publishers out there.\n\nThis R package intends to help users to obtain factual data from MDPI's journals, special issues and articles directly from their website (via web-scraping). Detailed information on functions and datasets can be found in the [Reference](reference.html) section.\n\nThe following section aims to provide a brief and approachable tutorial introducing users to the functionalities of the R package MDPIexploreR.\n\n## Installing MDPIexploreR\n\n```{r eval=FALSE}\ndevtools::install_github(\"pgomba/MDPI_explorer\")\nlibrary(MDPIexploreR)\n```\n\n```{r include=FALSE}\nlibrary(MDPIexploreR)\n```\n\n## Exploring journal articles\n\nObtaining a list of articles from a journal is easy thanks to the function `article_find()`. This function returns a vector of articles URLs. To do so, we just need to submit the journal code as a text string.\n\n```{r cache=TRUE}\n\nurls<-article_find(\"agriculture\")\n\nprint(paste(\"Articles found:\", length(urls)))\n\n```\n\nThe journal code name usually coincides with the journal title, but this is not always the case if the journal name is too long. To find the code name for your journal of interest check the dataset `MDPI_journals`, included in the package:\n\n```{r cache=TRUE}\nMDPIexploreR::MDPI_journals|>\n  head(10)\n```\n\n::: callout-note\nNote the code for the journal \"*Acoustics*\" matches the title of the journal, but the code for \"*Advances in Respiratory Medicine*\" is just the text string \"*arm*\".\n:::\n\nThe resulting vector from using `article_find()` (or any vector with scientific papers URLs), can then be combined with the function `article_info()`. This function will, for every article in the list, obtain receiving and accepting dates (to calculate turnaround times), obtain article type (e.g. editorial, review) and find out if it belongs to a special issue. Lets find information on 10 random articles from the journal \"Covid\", leaving 2 seconds between scraping iterations.\n\n```{r cache=TRUE, include=FALSE}\n\ninfo<-article_find(\"covid\") |>\n  article_info(sample_size=10,sleep=2)\n\n```\n\n```{r}\n# Show article type, turnaround time and if article is included in special issue\ninfo|>\n  dplyr::mutate(doi=gsub(\"https://www.mdpi.com/\",\"\",i))|> #To reduce output width\n  dplyr::select(doi,article_type,tat,issue_type)\n```\n\nBy default, sleep is two seconds. Reducing this number might cause the server to kick you out, specially when scraping large numbers of articles. sample_size, if blank, will iterate through the whole vector of articles.\n\n::: callout-important\nA stable internet connexion is recommended, specially for web scraping large numbers of papers\n:::\n\n::: callout-tip\nWeb scraping large amounts of URLs can be time consuming (2 seconds per paper, depending on delay) and many things can go wrong during the process (problematic URLs, being kicked out of the server...). My advice is to split large URL vectors in smaller ones.\n:::\n\n## Plotting article_info()\n\n`MDPIexploreR` provides with three functions to plot the results from `article_info()`. Lets load one of the data frames provided by the package first:\n\n```{r}\nagriculture_info<-MDPIexploreR::agriculture\n\nnrow(agriculture_info)\n```\n\n`summary_graph()` plots publications over time. The title of the journal must be provided:\n\n```{r eval=FALSE}\nsummary_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_summary_graph.png)\n\naverage_graph() plots average monthly turnaround times for the time period included in the dataset:\n\n```{r eval=FALSE}\naverage_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_average_graph.png)\n\n`issues_graph()` classifies articles depending on where they were published, including special issues\n\n```{r eval=FALSE}\nissues_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_issues_graph.png)\n\nLastly, types_graphs() plots a classification of articles depending on their type (editorial, review, etc)\n\n```{r eval=FALSE}\ntypes_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_types_graph.png)\n\nAll plots can be saved via `ggsave()`\n\n## Exploring special issues and guest editors\n\nSimilar to `article_find()`, the function `special_issue_find()` outputs a vector with all special issues available in the target journal. By default, it retrieves all CLOSED special issues, but this can be adjusted with the parameter type.\n\n```{r message=FALSE}\n\n# Creates a vector with all CLOSED special issues from the journal Covid\nURLs<-special_issue_find(\"covid\")\nprint(paste(\"Closed Special Issues:\",length(URLs)))\n\n# Creates a vector with all special issues from the journal Covid\nURLs<-special_issue_find(\"covid\", type=\"all\")\nprint(paste(\"All Special Issues:\",length(URLs)))\n\n# Creates a vector with all open issues from the journal Covid\nURLs<-special_issue_find(\"covid\", type=\"open\")\nprint(paste(\"Open Special Issues:\",length(URLs)))\n\n# Creates a vector with all closed special issues from the journal Covid for the year 2023\nURLs<-special_issue_find(\"covid\", type=\"closed\",years=2024)\nprint(paste(\"Open Special Issues:\",length(URLs)))\n\n# Creates a vector with all closed special issues from the journal Covid for the year 2023 and 2024\nURLs<-special_issue_find(\"covid\", type=\"closed\",years=c(2023,2024))\nprint(paste(\"Open Special Issues:\",length(URLs)))\n\n```\n\n`guest_editor_info()` uses then the vector produced by special_issue_find() to look for proportion of articles in special issues where the guest editors were involved and differences between special issue closing time and last article submitted. This function is inspired by [MA Oviedo-García](https://twitter.com/maoviedogarcia) work on MDPI's special issues. Similar to article_info(), it allows to select only a sample of special issues and set up a delay between scraping iterations.\n\n```{r eval=FALSE}\n\nURLs<-special_issue_find(\"covid\")\n\n# Extract data from all URLs, iterating every 3 seconds\nguest_editor_info (URLs, sleep=3)\n\n# Extract data from 2 URLs, iterating every 2 seconds (default)\nguest_editor_info (URLs, sample_size=2)\n\n```\n","srcMarkdownNoYaml":"\n\n<!-- badges: start -->\n[![](https://github.com/pgomba/MDPI_explorer/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/pgomba/MDPI_explorer/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n## Background {.unnumbered}\n\nEver changing scientific publishing strategies shape academic communications.\n\nTo date, MDPI is the largest publisher of Open Access articles in the world and top-3 overall publisher (Right after Elsevier and SpringerNature). \"[The Strain on Scientific Publishing](https://arxiv.org/abs/2309.15884)\" highlights them as a frequent outlier for several metrics, but also as one of the most transparent major publishers out there.\n\nThis R package intends to help users to obtain factual data from MDPI's journals, special issues and articles directly from their website (via web-scraping). Detailed information on functions and datasets can be found in the [Reference](reference.html) section.\n\nThe following section aims to provide a brief and approachable tutorial introducing users to the functionalities of the R package MDPIexploreR.\n\n## Installing MDPIexploreR\n\n```{r eval=FALSE}\ndevtools::install_github(\"pgomba/MDPI_explorer\")\nlibrary(MDPIexploreR)\n```\n\n```{r include=FALSE}\nlibrary(MDPIexploreR)\n```\n\n## Exploring journal articles\n\nObtaining a list of articles from a journal is easy thanks to the function `article_find()`. This function returns a vector of articles URLs. To do so, we just need to submit the journal code as a text string.\n\n```{r cache=TRUE}\n\nurls<-article_find(\"agriculture\")\n\nprint(paste(\"Articles found:\", length(urls)))\n\n```\n\nThe journal code name usually coincides with the journal title, but this is not always the case if the journal name is too long. To find the code name for your journal of interest check the dataset `MDPI_journals`, included in the package:\n\n```{r cache=TRUE}\nMDPIexploreR::MDPI_journals|>\n  head(10)\n```\n\n::: callout-note\nNote the code for the journal \"*Acoustics*\" matches the title of the journal, but the code for \"*Advances in Respiratory Medicine*\" is just the text string \"*arm*\".\n:::\n\nThe resulting vector from using `article_find()` (or any vector with scientific papers URLs), can then be combined with the function `article_info()`. This function will, for every article in the list, obtain receiving and accepting dates (to calculate turnaround times), obtain article type (e.g. editorial, review) and find out if it belongs to a special issue. Lets find information on 10 random articles from the journal \"Covid\", leaving 2 seconds between scraping iterations.\n\n```{r cache=TRUE, include=FALSE}\n\ninfo<-article_find(\"covid\") |>\n  article_info(sample_size=10,sleep=2)\n\n```\n\n```{r}\n# Show article type, turnaround time and if article is included in special issue\ninfo|>\n  dplyr::mutate(doi=gsub(\"https://www.mdpi.com/\",\"\",i))|> #To reduce output width\n  dplyr::select(doi,article_type,tat,issue_type)\n```\n\nBy default, sleep is two seconds. Reducing this number might cause the server to kick you out, specially when scraping large numbers of articles. sample_size, if blank, will iterate through the whole vector of articles.\n\n::: callout-important\nA stable internet connexion is recommended, specially for web scraping large numbers of papers\n:::\n\n::: callout-tip\nWeb scraping large amounts of URLs can be time consuming (2 seconds per paper, depending on delay) and many things can go wrong during the process (problematic URLs, being kicked out of the server...). My advice is to split large URL vectors in smaller ones.\n:::\n\n## Plotting article_info()\n\n`MDPIexploreR` provides with three functions to plot the results from `article_info()`. Lets load one of the data frames provided by the package first:\n\n```{r}\nagriculture_info<-MDPIexploreR::agriculture\n\nnrow(agriculture_info)\n```\n\n`summary_graph()` plots publications over time. The title of the journal must be provided:\n\n```{r eval=FALSE}\nsummary_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_summary_graph.png)\n\naverage_graph() plots average monthly turnaround times for the time period included in the dataset:\n\n```{r eval=FALSE}\naverage_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_average_graph.png)\n\n`issues_graph()` classifies articles depending on where they were published, including special issues\n\n```{r eval=FALSE}\nissues_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_issues_graph.png)\n\nLastly, types_graphs() plots a classification of articles depending on their type (editorial, review, etc)\n\n```{r eval=FALSE}\ntypes_graph(agriculture_info, journal=\"Agriculture\")\n```\n\n![](images/agriculture_types_graph.png)\n\nAll plots can be saved via `ggsave()`\n\n## Exploring special issues and guest editors\n\nSimilar to `article_find()`, the function `special_issue_find()` outputs a vector with all special issues available in the target journal. By default, it retrieves all CLOSED special issues, but this can be adjusted with the parameter type.\n\n```{r message=FALSE}\n\n# Creates a vector with all CLOSED special issues from the journal Covid\nURLs<-special_issue_find(\"covid\")\nprint(paste(\"Closed Special Issues:\",length(URLs)))\n\n# Creates a vector with all special issues from the journal Covid\nURLs<-special_issue_find(\"covid\", type=\"all\")\nprint(paste(\"All Special Issues:\",length(URLs)))\n\n# Creates a vector with all open issues from the journal Covid\nURLs<-special_issue_find(\"covid\", type=\"open\")\nprint(paste(\"Open Special Issues:\",length(URLs)))\n\n# Creates a vector with all closed special issues from the journal Covid for the year 2023\nURLs<-special_issue_find(\"covid\", type=\"closed\",years=2024)\nprint(paste(\"Open Special Issues:\",length(URLs)))\n\n# Creates a vector with all closed special issues from the journal Covid for the year 2023 and 2024\nURLs<-special_issue_find(\"covid\", type=\"closed\",years=c(2023,2024))\nprint(paste(\"Open Special Issues:\",length(URLs)))\n\n```\n\n`guest_editor_info()` uses then the vector produced by special_issue_find() to look for proportion of articles in special issues where the guest editors were involved and differences between special issue closing time and last article submitted. This function is inspired by [MA Oviedo-García](https://twitter.com/maoviedogarcia) work on MDPI's special issues. Similar to article_info(), it allows to select only a sample of special issues and set up a delay between scraping iterations.\n\n```{r eval=FALSE}\n\nURLs<-special_issue_find(\"covid\")\n\n# Extract data from all URLs, iterating every 3 seconds\nguest_editor_info (URLs, sleep=3)\n\n# Extract data from 2 URLs, iterating every 2 seconds (default)\nguest_editor_info (URLs, sample_size=2)\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":{"light":"cosmo","dark":"darkly"},"title":"Get started! A guide to MDPIexploreR"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}